{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "01e283c5-6ba3-4470-b211-54c968cae01a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspark==3.4.2 in /opt/anaconda3/lib/python3.11/site-packages (3.4.2)\n",
      "Requirement already satisfied: pandas in /opt/anaconda3/lib/python3.11/site-packages (2.1.1)\n",
      "Requirement already satisfied: pyarrow in /opt/anaconda3/lib/python3.11/site-packages (14.0.2)\n",
      "Requirement already satisfied: py4j==0.10.9.7 in /opt/anaconda3/lib/python3.11/site-packages (from pyspark==3.4.2) (0.10.9.7)\n",
      "Requirement already satisfied: numpy>=1.23.2 in /opt/anaconda3/lib/python3.11/site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/anaconda3/lib/python3.11/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/lib/python3.11/site-packages (from pandas) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/anaconda3/lib/python3.11/site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "# Run this once in a notebook cell if PySpark isn't installed\n",
    "!pip install pyspark==3.4.2 pandas pyarrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e2f69bd9-2338-4322-a586-4c9478bd8ebc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/08/12 21:22:14 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://10.0.0.210:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.4.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>RetailRocket-Intent-Notebook</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x1136deb10>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (SparkSession.builder\n",
    "         .appName(\"RetailRocket-Intent-Notebook\")\n",
    "         .config(\"spark.sql.session.timeZone\", \"UTC\")\n",
    "         .getOrCreate())\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "96aa8f00-e3e4-4862-b7c0-84d7e5c639cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F, types as T\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler, StandardScaler\n",
    "from pyspark.ml.classification import LogisticRegression, GBTClassifier\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "DATA_DIR = \"/Users/ernestgaisie/Desktop/Big Data Proj/data/archive\"    # <-- change if your CSVs live elsewhere\n",
    "GOLD_DIR = \"gold\"       # outputs will be written here (Parquet)\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"200\")  # adjust for your machine/cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8f6a9441-b253-4428-bacb-a1bc7ee32cb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Events count: 2756101\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------+\n",
      "|      event|  count|\n",
      "+-----------+-------+\n",
      "|transaction|  22457|\n",
      "|  addtocart|  69332|\n",
      "|       view|2664312|\n",
      "+-----------+-------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 6:=======>                                                   (1 + 7) / 8]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+-----------------------+\n",
      "|min_ts                 |max_ts                 |\n",
      "+-----------------------+-----------------------+\n",
      "|2015-05-03 03:00:04.384|2015-09-18 02:59:47.788|\n",
      "+-----------------------+-----------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "events_schema = T.StructType([\n",
    "    T.StructField(\"timestamp\", T.LongType(), False),\n",
    "    T.StructField(\"visitorid\", T.StringType(), False),\n",
    "    T.StructField(\"event\", T.StringType(), False),\n",
    "    T.StructField(\"itemid\", T.IntegerType(), False),\n",
    "    T.StructField(\"transactionid\", T.StringType(), True)\n",
    "])\n",
    "\n",
    "props_schema = T.StructType([\n",
    "    T.StructField(\"timestamp\", T.LongType(), False),\n",
    "    T.StructField(\"itemid\", T.IntegerType(), False),\n",
    "    T.StructField(\"property\", T.StringType(), False),\n",
    "    T.StructField(\"value\", T.StringType(), True)\n",
    "])\n",
    "\n",
    "cat_schema = T.StructType([\n",
    "    T.StructField(\"categoryid\", T.IntegerType(), False),\n",
    "    T.StructField(\"parentid\", T.IntegerType(), True)\n",
    "])\n",
    "\n",
    "events_raw = spark.read.csv(f\"{DATA_DIR}/events.csv\", header=True, schema=events_schema)\n",
    "p1 = spark.read.csv(f\"{DATA_DIR}/item_properties_part1.csv\", header=True, schema=props_schema)\n",
    "p2 = spark.read.csv(f\"{DATA_DIR}/item_properties_part2.csv\", header=True, schema=props_schema)\n",
    "cats = spark.read.csv(f\"{DATA_DIR}/category_tree.csv\", header=True, schema=cat_schema)\n",
    "\n",
    "events = (events_raw\n",
    "          .withColumn(\"ts\", F.to_timestamp((F.col(\"timestamp\")/1000).cast(\"timestamp\")))\n",
    "          .drop(\"timestamp\")\n",
    "          .filter(F.col(\"event\").isin(\"view\",\"addtocart\",\"transaction\")))\n",
    "\n",
    "print(\"Events count:\", events.count())\n",
    "events.groupBy(\"event\").count().show()\n",
    "events.select(F.min(\"ts\").alias(\"min_ts\"), F.max(\"ts\").alias(\"max_ts\")).show(truncate=False)\n",
    "\n",
    "# (Optional during debugging) downsample to speed up iteration:\n",
    "# events = events.limit(100_000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2422ae73-caf9-409e-8167-50cc2b3fb06d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+\n",
      "|property  |count |\n",
      "+----------+------+\n",
      "|categoryid|417053|\n",
      "|112       |417053|\n",
      "|888       |417053|\n",
      "|364       |417053|\n",
      "|790       |417053|\n",
      "|283       |417053|\n",
      "|159       |417053|\n",
      "|available |417053|\n",
      "|764       |417053|\n",
      "|678       |417019|\n",
      "|917       |416171|\n",
      "|202       |414217|\n",
      "|6         |409065|\n",
      "|776       |407305|\n",
      "|839       |396644|\n",
      "|227       |328096|\n",
      "|698       |274747|\n",
      "|689       |211791|\n",
      "|28        |169926|\n",
      "|928       |150121|\n",
      "|348       |110602|\n",
      "|810       |103135|\n",
      "|1036      |102592|\n",
      "|713       |92762 |\n",
      "|19        |74408 |\n",
      "|400       |54823 |\n",
      "|434       |54141 |\n",
      "|46        |54141 |\n",
      "|38        |54141 |\n",
      "|243       |54141 |\n",
      "+----------+------+\n",
      "only showing top 30 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 17:===================================================>    (11 + 1) / 12]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+---------+-----+\n",
      "|itemid|categoryid|available|price|\n",
      "+------+----------+---------+-----+\n",
      "|26    |1503      |0        |null |\n",
      "|27    |769       |0        |null |\n",
      "|28    |967       |0        |null |\n",
      "|31    |1338      |0        |null |\n",
      "|34    |330       |0        |null |\n",
      "+------+----------+---------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "props = (p1.unionByName(p2)\n",
    "         .withColumn(\"ts\", F.to_timestamp((F.col(\"timestamp\")/1000).cast(\"timestamp\")))\n",
    "         .drop(\"timestamp\"))\n",
    "\n",
    "w_latest = Window.partitionBy(\"itemid\",\"property\").orderBy(F.col(\"ts\").desc())\n",
    "props_latest = (props\n",
    "                .withColumn(\"rn\", F.row_number().over(w_latest))\n",
    "                .filter(\"rn = 1\")\n",
    "                .drop(\"rn\",\"ts\"))\n",
    "\n",
    "# Inspect property keys to decide what to pivot\n",
    "props_latest.groupBy(\"property\").count().orderBy(F.desc(\"count\")).show(30, truncate=False)\n",
    "\n",
    "# Pivot a starter set â€” edit this list after inspecting the output above\n",
    "pivot_keys = [\"categoryid\", \"available\", \"price\"]\n",
    "\n",
    "items_wide = (props_latest\n",
    "              .groupBy(\"itemid\")\n",
    "              .pivot(\"property\", pivot_keys)\n",
    "              .agg(F.first(\"value\")))\n",
    "\n",
    "# Cast present columns safely\n",
    "if \"categoryid\" in items_wide.columns:\n",
    "    items_wide = items_wide.withColumn(\"categoryid\", F.col(\"categoryid\").cast(\"int\"))\n",
    "\n",
    "if \"available\" in items_wide.columns:\n",
    "    items_wide = items_wide.withColumn(\"available\", F.col(\"available\").cast(\"int\"))\n",
    "\n",
    "if \"price\" in items_wide.columns:\n",
    "    items_wide = items_wide.withColumn(\"price\",\n",
    "        F.regexp_replace(F.col(\"price\"), r\"[^0-9.]\", \"\").cast(\"double\"))\n",
    "\n",
    "items_wide.limit(5).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "16845e47-1dac-4b4d-9cdf-843f77c137c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/08/12 21:23:13 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/08/12 21:23:13 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/08/12 21:23:13 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/08/12 21:23:13 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/08/12 21:23:13 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/08/12 21:23:13 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/08/12 21:23:13 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/08/12 21:23:13 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "[Stage 26:==============>                                           (2 + 6) / 8]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------------------+-----------------------+-----+----+------------+-------------------+\n",
      "|session_id|session_start          |session_end            |views|adds|unique_items|duration_min       |\n",
      "+----------+-----------------------+-----------------------+-----+----+------------+-------------------+\n",
      "|1052830-1 |2015-05-14 18:30:59.609|2015-05-14 18:30:59.609|1    |0   |1           |0.0                |\n",
      "|1093878-1 |2015-08-15 06:31:35.906|2015-08-15 06:31:35.906|1    |0   |1           |0.0                |\n",
      "|178444-1  |2015-08-14 03:19:45.87 |2015-08-14 03:19:56.438|2    |0   |2           |0.18333333333333332|\n",
      "|212092-1  |2015-07-24 04:03:25.237|2015-07-24 04:15:18.298|5    |0   |4           |11.883333333333333 |\n",
      "|233257-1  |2015-06-25 04:58:47.733|2015-06-25 04:58:47.733|1    |0   |1           |0.0                |\n",
      "+----------+-----------------------+-----------------------+-----+----+------------+-------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "w_user = Window.partitionBy(\"visitorid\").orderBy(\"ts\")\n",
    "\n",
    "events2 = (events\n",
    "  .withColumn(\"prev_ts\", F.lag(\"ts\").over(w_user))\n",
    "  .withColumn(\"gap_min\", F.when(F.col(\"prev_ts\").isNull(), 1e9)\n",
    "                          .otherwise((F.col(\"ts\").cast(\"long\") - F.col(\"prev_ts\").cast(\"long\"))/60.0))\n",
    "  .withColumn(\"new_sess\", F.when(F.col(\"gap_min\") > 30, 1).otherwise(0))\n",
    "  .withColumn(\"session_seq\", F.sum(\"new_sess\").over(w_user))\n",
    "  .withColumn(\"session_id\", F.concat_ws(\"-\", F.col(\"visitorid\"), F.col(\"session_seq\")))\n",
    "  .drop(\"prev_ts\",\"gap_min\",\"new_sess\",\"session_seq\")\n",
    ")\n",
    "\n",
    "events3 = events2.join(items_wide, \"itemid\", \"left\")\n",
    "\n",
    "sess_aggs = (events3.groupBy(\"session_id\")\n",
    "  .agg(\n",
    "    F.min(\"ts\").alias(\"session_start\"),\n",
    "    F.max(\"ts\").alias(\"session_end\"),\n",
    "    F.count(F.when(F.col(\"event\")==\"view\", True)).alias(\"views\"),\n",
    "    F.count(F.when(F.col(\"event\")==\"addtocart\", True)).alias(\"adds\"),\n",
    "    F.countDistinct(\"itemid\").alias(\"unique_items\")\n",
    "  )\n",
    "  .withColumn(\"duration_min\", (F.col(\"session_end\").cast(\"long\") - F.col(\"session_start\").cast(\"long\"))/60.0)\n",
    ")\n",
    "\n",
    "sess_aggs.limit(5).show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5d4b3e30-0c9a-424a-9e4d-da87b4d14d9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+\n",
      "|label|  count|\n",
      "+-----+-------+\n",
      "|    1|  21794|\n",
      "|    0|2326049|\n",
      "+-----+-------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "touch = (events3\n",
    "         .filter(F.col(\"event\").isin(\"view\",\"addtocart\",\"transaction\"))\n",
    "         .select(\"session_id\",\"visitorid\",\"ts\",\"itemid\",\"event\"))\n",
    "\n",
    "purchased = (touch.filter(\"event = 'transaction'\")\n",
    "             .select(\"session_id\",\"itemid\")\n",
    "             .dropDuplicates()\n",
    "             .withColumn(\"label\", F.lit(1)))\n",
    "\n",
    "w_last = Window.partitionBy(\"session_id\",\"itemid\").orderBy(F.col(\"ts\").desc())\n",
    "cand = (touch.withColumn(\"rn\", F.row_number().over(w_last))\n",
    "        .filter(\"rn = 1\").drop(\"rn\",\"event\",\"ts\"))\n",
    "\n",
    "labeled = cand.join(purchased, [\"session_id\",\"itemid\"], \"left\").fillna({\"label\":0})\n",
    "labeled.groupBy(\"label\").count().show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f6109994-751a-4ec5-8056-a0cb2f5a48d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/08/12 21:23:39 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/08/12 21:23:39 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/08/12 21:23:39 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/08/12 21:23:39 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/08/12 21:23:39 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/08/12 21:23:39 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/08/12 21:23:44 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/08/12 21:23:44 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/08/12 21:23:44 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/08/12 21:23:44 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/08/12 21:23:44 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/08/12 21:23:44 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/08/12 21:23:44 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/08/12 21:23:44 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------+------+----------+-----+----+------------+\n",
      "|label|visitorid|itemid|categoryid|views|adds|unique_items|\n",
      "+-----+---------+------+----------+-----+----+------------+\n",
      "|    0|  1000012|128596|      1503|    1|   0|           1|\n",
      "|    0|  1000001|202293|       683|    3|   0|           3|\n",
      "|    0|  1000046|219657|      1504|    2|   0|           2|\n",
      "|    0|     1000|248975|       142|    1|   0|           1|\n",
      "|    0|  1000010| 25325|       628|    1|   0|           1|\n",
      "+-----+---------+------+----------+-----+----+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# User-level features\n",
    "user_roll = (events3.groupBy(\"visitorid\")\n",
    "  .agg(F.count(\"*\").alias(\"u_events\"),\n",
    "       F.count(F.when(F.col(\"event\")==\"view\", True)).alias(\"u_views\"),\n",
    "       F.count(F.when(F.col(\"event\")==\"addtocart\", True)).alias(\"u_adds\"),\n",
    "       F.count(F.when(F.col(\"event\")==\"transaction\", True)).alias(\"u_txn\")))\n",
    "\n",
    "# Item-level features\n",
    "item_pop = (events3.groupBy(\"itemid\")\n",
    "  .agg(F.count(F.when(F.col(\"event\")==\"view\", True)).alias(\"i_views\"),\n",
    "       F.count(F.when(F.col(\"event\")==\"addtocart\", True)).alias(\"i_adds\"),\n",
    "       F.count(F.when(F.col(\"event\")==\"transaction\", True)).alias(\"i_txn\")))\n",
    "\n",
    "# Assemble training frame\n",
    "xy = (labeled\n",
    "  .join(sess_aggs, \"session_id\")\n",
    "  .join(user_roll, \"visitorid\", \"left\")\n",
    "  .join(item_pop, \"itemid\", \"left\")\n",
    "  .join(items_wide.select(\"itemid\",\"categoryid\"), \"itemid\", \"left\"))\n",
    "\n",
    "# Fill NA & cast types\n",
    "for c in [\"u_events\",\"u_views\",\"u_adds\",\"u_txn\",\n",
    "          \"i_views\",\"i_adds\",\"i_txn\",\n",
    "          \"duration_min\",\"views\",\"adds\",\"unique_items\"]:\n",
    "    xy = xy.fillna({c: 0})\n",
    "\n",
    "xy = xy.withColumn(\"categoryid\", F.col(\"categoryid\").cast(\"int\"))\n",
    "\n",
    "xy.select(\"label\",\"visitorid\",\"itemid\",\"categoryid\",\"views\",\"adds\",\"unique_items\").limit(5).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8eb8fac1-fd28-49e8-a9a8-cd54a4673f18",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/08/12 21:24:07 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/08/12 21:24:07 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/08/12 21:24:07 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/08/12 21:24:07 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/08/12 21:24:07 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "[Stage 100:==================================================>    (11 + 1) / 12]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------+------+----------+-----+----+------------+\n",
      "|label|visitorid|itemid|categoryid|views|adds|unique_items|\n",
      "+-----+---------+------+----------+-----+----+------------+\n",
      "|    0|  1000012|128596|      1503|    1|   0|           1|\n",
      "|    0|  1000001|202293|       683|    3|   0|           3|\n",
      "|    0|  1000046|219657|      1504|    2|   0|           2|\n",
      "|    0|     1000|248975|       142|    1|   0|           1|\n",
      "|    0|  1000010| 25325|       628|    1|   0|           1|\n",
      "+-----+---------+------+----------+-----+----+------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# User-level features\n",
    "user_roll = (events3.groupBy(\"visitorid\")\n",
    "  .agg(F.count(\"*\").alias(\"u_events\"),\n",
    "       F.count(F.when(F.col(\"event\")==\"view\", True)).alias(\"u_views\"),\n",
    "       F.count(F.when(F.col(\"event\")==\"addtocart\", True)).alias(\"u_adds\"),\n",
    "       F.count(F.when(F.col(\"event\")==\"transaction\", True)).alias(\"u_txn\")))\n",
    "\n",
    "# Item-level features\n",
    "item_pop = (events3.groupBy(\"itemid\")\n",
    "  .agg(F.count(F.when(F.col(\"event\")==\"view\", True)).alias(\"i_views\"),\n",
    "       F.count(F.when(F.col(\"event\")==\"addtocart\", True)).alias(\"i_adds\"),\n",
    "       F.count(F.when(F.col(\"event\")==\"transaction\", True)).alias(\"i_txn\")))\n",
    "\n",
    "# Assemble training frame\n",
    "xy = (labeled\n",
    "  .join(sess_aggs, \"session_id\")\n",
    "  .join(user_roll, \"visitorid\", \"left\")\n",
    "  .join(item_pop, \"itemid\", \"left\")\n",
    "  .join(items_wide.select(\"itemid\",\"categoryid\"), \"itemid\", \"left\"))\n",
    "\n",
    "# Fill NA & cast types\n",
    "for c in [\"u_events\",\"u_views\",\"u_adds\",\"u_txn\",\n",
    "          \"i_views\",\"i_adds\",\"i_txn\",\n",
    "          \"duration_min\",\"views\",\"adds\",\"unique_items\"]:\n",
    "    xy = xy.fillna({c: 0})\n",
    "\n",
    "xy = xy.withColumn(\"categoryid\", F.col(\"categoryid\").cast(\"int\"))\n",
    "\n",
    "xy.select(\"label\",\"visitorid\",\"itemid\",\"categoryid\",\"views\",\"adds\",\"unique_items\").limit(5).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "64c19811-8b20-43d5-818c-65b60cdebe95",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cutoff epoch: 1439912838.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 181:======================================>                  (6 + 3) / 9]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train rows: 1877612 Test rows: 470231\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# 1) Convert the timestamp column into a numeric (epoch seconds)\n",
    "xy = xy.withColumn(\"session_start_ts\", F.col(\"session_start\").cast(\"long\"))\n",
    "\n",
    "# 2) Remove any rows without a session_start timestamp\n",
    "xy = xy.filter(F.col(\"session_start_ts\").isNotNull())\n",
    "\n",
    "# 3) Compute the 80th percentile on the numeric column\n",
    "q = xy.approxQuantile(\"session_start_ts\", [0.8], 0.001)[0]\n",
    "\n",
    "# 4) Filter into train/test sets using that numeric cutoff\n",
    "train = xy.filter(F.col(\"session_start_ts\") <= q)\n",
    "test  = xy.filter(F.col(\"session_start_ts\")  > q)\n",
    "\n",
    "print(\"Cutoff epoch:\", q)\n",
    "print(\"Train rows:\", train.count(), \"Test rows:\", test.count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "44d642d8-0960-4a26-9655-5e8c3bc0b021",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 263:>                                                        (0 + 8) / 9]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train balanced -> pos: 17376 neg (sampled): 52100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# Helpful if positives are rare. You can skip this if metrics look fine.\n",
    "pos = train.filter(\"label = 1\")\n",
    "neg = train.filter(\"label = 0\")\n",
    "pos_cnt, neg_cnt = pos.count(), neg.count()\n",
    "\n",
    "# Downsample negatives to ~3x positives (tune as needed)\n",
    "frac = min(1.0, (3.0 * max(1, pos_cnt)) / max(1, neg_cnt))\n",
    "train_bal = pos.unionByName(neg.sample(False, frac, seed=42))\n",
    "\n",
    "print(\"train balanced -> pos:\", pos_cnt, \"neg (sampled):\", train_bal.filter(\"label=0\").count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4fc10f3c-d5af-4f54-91af-519902480608",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/08/12 21:26:03 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/08/12 21:26:03 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/08/12 21:26:03 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/08/12 21:26:03 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/08/12 21:26:03 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/08/12 21:26:03 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/08/12 21:26:03 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/08/12 21:26:03 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/08/12 21:26:11 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/08/12 21:26:11 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/08/12 21:26:11 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/08/12 21:26:11 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/08/12 21:26:11 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/08/12 21:26:11 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/08/12 21:26:11 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/08/12 21:26:12 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/08/12 21:26:12 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/08/12 21:26:12 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/08/12 21:26:12 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/08/12 21:26:12 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/08/12 21:26:12 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/08/12 21:26:12 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/08/12 21:26:12 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/08/12 21:26:12 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/08/12 21:26:37 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/08/12 21:26:37 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/08/12 21:26:37 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/08/12 21:26:37 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/08/12 21:26:37 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/08/12 21:26:37 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/08/12 21:26:37 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/08/12 21:26:37 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/08/12 21:26:45 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/08/12 21:27:13 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/08/12 21:27:13 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/08/12 21:27:13 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/08/12 21:27:13 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/08/12 21:27:13 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/08/12 21:27:13 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/08/12 21:27:13 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/08/12 21:27:13 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/08/12 21:27:22 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/08/12 21:27:22 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/08/12 21:27:33 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n",
      "25/08/12 21:27:33 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.VectorBLAS\n",
      "25/08/12 21:28:10 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/08/12 21:28:10 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/08/12 21:28:11 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/08/12 21:28:11 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/08/12 21:28:11 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/08/12 21:28:11 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/08/12 21:28:44 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/08/12 21:28:53 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/08/12 21:28:53 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/08/12 21:28:53 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/08/12 21:28:53 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/08/12 21:28:53 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/08/12 21:28:53 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/08/12 21:28:53 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/08/12 21:28:53 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/08/12 21:28:53 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/08/12 21:28:53 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/08/12 21:28:53 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/08/12 21:28:53 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/08/12 21:29:21 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/08/12 21:29:21 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/08/12 21:29:28 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/08/12 21:29:28 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/08/12 21:29:28 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/08/12 21:29:28 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/08/12 21:29:28 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/08/12 21:29:28 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/08/12 21:29:28 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/08/12 21:29:28 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/08/12 21:29:28 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/08/12 21:29:28 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/08/12 21:29:28 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/08/12 21:29:28 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/08/12 21:29:29 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/08/12 21:29:29 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/08/12 21:29:29 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/08/12 21:29:29 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/08/12 21:29:29 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/08/12 21:29:29 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/08/12 21:29:29 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR AUROC: 0.9135326296823831\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/08/12 21:30:02 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/08/12 21:30:02 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/08/12 21:30:02 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/08/12 21:30:02 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR AUPRC: 0.12908344593026533\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/08/12 21:30:30 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/08/12 21:30:30 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/08/12 21:30:38 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/08/12 21:30:38 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/08/12 21:30:39 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/08/12 21:30:39 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/08/12 21:30:39 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/08/12 21:30:39 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/08/12 21:30:39 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/08/12 21:30:39 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/08/12 21:30:39 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/08/12 21:30:39 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/08/12 21:30:45 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/08/12 21:30:45 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/08/12 21:30:45 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/08/12 21:30:45 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+------+\n",
      "|label|pred| count|\n",
      "+-----+----+------+\n",
      "|    0|   0|451529|\n",
      "|    0|   1| 14284|\n",
      "|    1|   0|  2010|\n",
      "|    1|   1|  2408|\n",
      "+-----+----+------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/08/12 21:31:13 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/08/12 21:31:13 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR AUPRC (using p1): 0.1297222825855104\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler, StandardScaler\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "cat_cols = [\"categoryid\"]\n",
    "num_cols = [\"views\",\"adds\",\"unique_items\",\"duration_min\",\n",
    "            \"u_events\",\"u_views\",\"u_adds\",\"u_txn\",\n",
    "            \"i_views\",\"i_adds\",\"i_txn\"]\n",
    "\n",
    "indexers = [StringIndexer(inputCol=c, outputCol=f\"{c}_idx\", handleInvalid=\"keep\") for c in cat_cols]\n",
    "ohe = OneHotEncoder(inputCols=[f\"{c}_idx\" for c in cat_cols],\n",
    "                    outputCols=[f\"{c}_oh\" for c in cat_cols])\n",
    "assembler = VectorAssembler(inputCols=num_cols + [f\"{c}_oh\" for c in cat_cols],\n",
    "                            outputCol=\"features_raw\")\n",
    "scaler = StandardScaler(inputCol=\"features_raw\", outputCol=\"features\", withStd=True)\n",
    "\n",
    "lr = LogisticRegression(featuresCol=\"features\", labelCol=\"label\")\n",
    "\n",
    "pipe_lr = Pipeline(stages=[*indexers, ohe, assembler, scaler, lr])\n",
    "\n",
    "# Use train_bal if you ran the balancing cell; otherwise use train\n",
    "model_lr = pipe_lr.fit(train_bal if 'train_bal' in globals() else train)\n",
    "preds_lr  = model_lr.transform(test)\n",
    "\n",
    "e_roc = BinaryClassificationEvaluator(labelCol=\"label\", metricName=\"areaUnderROC\")\n",
    "e_pr  = BinaryClassificationEvaluator(labelCol=\"label\", metricName=\"areaUnderPR\")\n",
    "\n",
    "print(\"LR AUROC:\", e_roc.evaluate(preds_lr))\n",
    "print(\"LR AUPRC:\", e_pr.evaluate(preds_lr))\n",
    "\n",
    "# Confusion matrix at 0.5\n",
    "from pyspark.ml.functions import vector_to_array\n",
    "\n",
    "# Turn probability vector into an array and take class-1 prob\n",
    "preds_lr = preds_lr.withColumn(\"prob_arr\", vector_to_array(\"probability\"))\n",
    "preds_lr = preds_lr.withColumn(\"p1\", F.col(\"prob_arr\")[1])\n",
    "\n",
    "# Confusion matrix at 0.5\n",
    "preds_bin = preds_lr.withColumn(\"pred\", (F.col(\"p1\") >= 0.5).cast(\"int\"))\n",
    "preds_bin.groupBy(\"label\",\"pred\").count().orderBy(\"label\",\"pred\").show()\n",
    "\n",
    "# (Optional) Re-evaluate using p1\n",
    "e_pr_p = BinaryClassificationEvaluator(labelCol=\"label\", rawPredictionCol=\"p1\", metricName=\"areaUnderPR\")\n",
    "print(\"LR AUPRC (using p1):\", e_pr_p.evaluate(preds_lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0d69a204-a326-4c44-80a5-dfd39de7233b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/08/12 21:32:54 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/08/12 21:32:54 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/08/12 21:32:54 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/08/12 21:32:54 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/08/12 21:32:54 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/08/12 21:32:54 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/08/12 21:32:54 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/08/12 21:32:54 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/08/12 21:32:55 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/08/12 21:32:55 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/08/12 21:33:26 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/08/12 21:33:34 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/08/12 21:33:34 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/08/12 21:33:34 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/08/12 21:33:34 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/08/12 21:33:34 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/08/12 21:33:34 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/08/12 21:33:34 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/08/12 21:33:34 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/08/12 21:33:36 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/08/12 21:34:27 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/08/12 21:34:27 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/08/12 21:34:27 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/08/12 21:34:27 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/08/12 21:34:27 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/08/12 21:34:27 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/08/12 21:34:47 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/08/12 21:34:47 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/08/12 21:34:47 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/08/12 21:34:47 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/08/12 21:34:47 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/08/12 21:34:47 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/08/12 21:34:47 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/08/12 21:34:47 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/08/12 21:34:49 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/08/12 21:35:03 WARN MemoryStore: Not enough space to cache rdd_2534_3 in memory! (computed 29.7 MiB so far)\n",
      "25/08/12 21:35:03 WARN MemoryStore: Not enough space to cache rdd_2534_6 in memory! (computed 29.7 MiB so far)\n",
      "25/08/12 21:35:03 WARN MemoryStore: Not enough space to cache rdd_2534_7 in memory! (computed 29.7 MiB so far)\n",
      "25/08/12 21:35:03 WARN MemoryStore: Not enough space to cache rdd_2534_0 in memory! (computed 19.5 MiB so far)\n",
      "25/08/12 21:35:03 WARN MemoryStore: Not enough space to cache rdd_2534_2 in memory! (computed 19.5 MiB so far)\n",
      "25/08/12 21:35:03 WARN MemoryStore: Not enough space to cache rdd_2534_4 in memory! (computed 29.7 MiB so far)\n",
      "25/08/12 21:35:03 WARN BlockManager: Persisting block rdd_2534_3 to disk instead.\n",
      "25/08/12 21:35:03 WARN BlockManager: Persisting block rdd_2534_0 to disk instead.\n",
      "25/08/12 21:35:03 WARN BlockManager: Persisting block rdd_2534_7 to disk instead.\n",
      "25/08/12 21:35:03 WARN BlockManager: Persisting block rdd_2534_4 to disk instead.\n",
      "25/08/12 21:35:03 WARN BlockManager: Persisting block rdd_2534_6 to disk instead.\n",
      "25/08/12 21:35:03 WARN BlockManager: Persisting block rdd_2534_2 to disk instead.\n",
      "25/08/12 21:35:03 WARN MemoryStore: Not enough space to cache rdd_2534_5 in memory! (computed 29.7 MiB so far)\n",
      "25/08/12 21:35:03 WARN BlockManager: Persisting block rdd_2534_5 to disk instead.\n",
      "25/08/12 21:35:08 WARN DAGScheduler: Broadcasting large task binary with size 1118.1 KiB\n",
      "25/08/12 21:35:10 WARN DAGScheduler: Broadcasting large task binary with size 1310.1 KiB\n",
      "25/08/12 21:35:11 WARN DAGScheduler: Broadcasting large task binary with size 1540.3 KiB\n",
      "25/08/12 21:35:12 WARN DAGScheduler: Broadcasting large task binary with size 1805.6 KiB\n",
      "25/08/12 21:35:13 WARN DAGScheduler: Broadcasting large task binary with size 2.1 MiB\n",
      "25/08/12 21:35:14 WARN DAGScheduler: Broadcasting large task binary with size 2.4 MiB\n",
      "25/08/12 21:35:16 WARN DAGScheduler: Broadcasting large task binary with size 2.8 MiB\n",
      "25/08/12 21:35:17 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "25/08/12 21:35:19 WARN DAGScheduler: Broadcasting large task binary with size 3.7 MiB\n",
      "25/08/12 21:35:21 WARN DAGScheduler: Broadcasting large task binary with size 4.2 MiB\n",
      "25/08/12 21:35:23 WARN DAGScheduler: Broadcasting large task binary with size 4.8 MiB\n",
      "25/08/12 21:35:25 WARN DAGScheduler: Broadcasting large task binary with size 5.4 MiB\n",
      "25/08/12 21:35:27 WARN DAGScheduler: Broadcasting large task binary with size 6.0 MiB\n",
      "25/08/12 21:35:30 WARN DAGScheduler: Broadcasting large task binary with size 6.8 MiB\n",
      "25/08/12 21:35:33 WARN DAGScheduler: Broadcasting large task binary with size 7.5 MiB\n",
      "25/08/12 21:35:54 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/08/12 21:35:54 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/08/12 21:35:54 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/08/12 21:35:54 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/08/12 21:35:54 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/08/12 21:35:55 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/08/12 21:35:55 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/08/12 21:36:03 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/08/12 21:36:29 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/08/12 21:36:29 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/08/12 21:36:29 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/08/12 21:36:29 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/08/12 21:36:29 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/08/12 21:36:29 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/08/12 21:36:29 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/08/12 21:36:29 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/08/12 21:36:40 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/08/12 21:36:40 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/08/12 21:36:40 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/08/12 21:36:40 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/08/12 21:36:40 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/08/12 21:36:40 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/08/12 21:36:40 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/08/12 21:36:40 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/08/12 21:36:43 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/08/12 21:37:18 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/08/12 21:37:18 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/08/12 21:37:18 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/08/12 21:37:18 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/08/12 21:37:18 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/08/12 21:37:18 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/08/12 21:37:18 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/08/12 21:37:18 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/08/12 21:37:20 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/08/12 21:37:20 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/08/12 21:37:20 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/08/12 21:37:33 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF AUROC: 0.9959500823279116\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/08/12 21:38:04 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/08/12 21:38:04 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/08/12 21:38:05 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/08/12 21:38:05 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/08/12 21:38:05 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/08/12 21:38:05 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/08/12 21:38:05 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/08/12 21:38:05 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/08/12 21:38:10 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF AUPRC: 0.7005160690760708\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/08/12 21:38:43 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/08/12 21:38:43 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/08/12 21:38:48 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "25/08/12 21:38:49 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/08/12 21:38:49 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/08/12 21:38:49 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/08/12 21:38:49 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "[Stage 3023:=================================================>      (8 + 1) / 9]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+------+\n",
      "|label|pred| count|\n",
      "+-----+----+------+\n",
      "|    0|   0|456822|\n",
      "|    0|   1|  8991|\n",
      "|    1|   0|    34|\n",
      "|    1|   1|  4384|\n",
      "+-----+----+------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/08/12 21:38:57 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "                                                                                "
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier(featuresCol=\"features\", labelCol=\"label\",\n",
    "                            numTrees=200, maxDepth=20, maxBins=128,\n",
    "                            featureSubsetStrategy=\"auto\", subsamplingRate=0.8, seed=42)\n",
    "\n",
    "pipe_rf = Pipeline(stages=[*indexers, ohe, assembler, scaler, rf])\n",
    "\n",
    "model_rf = pipe_rf.fit(train_bal if 'train_bal' in globals() else train)\n",
    "preds_rf = model_rf.transform(test)\n",
    "\n",
    "preds_rf = preds_rf.withColumn(\"prob_arr\", vector_to_array(\"probability\")) \\\n",
    "                   .withColumn(\"p1\", F.col(\"prob_arr\")[1])\n",
    "\n",
    "print(\"RF AUROC:\", e_roc.evaluate(preds_rf))\n",
    "print(\"RF AUPRC:\", e_pr.evaluate(preds_rf))\n",
    "\n",
    "preds_rf.withColumn(\"pred\", (F.col(\"p1\") >= 0.5).cast(\"int\")) \\\n",
    "        .groupBy(\"label\",\"pred\").count().orderBy(\"label\",\"pred\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "76194d9f-9daa-4b07-b5ea-8828e1680ae2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest pipeline saved to: gold/models/rf_pipeline\n"
     ]
    }
   ],
   "source": [
    "# Directory to store the model\n",
    "GOLD_DIR = \"gold\"\n",
    "\n",
    "# Save the full pipeline (transformations + RF)\n",
    "model_rf.write().overwrite().save(f\"{GOLD_DIR}/models/rf_pipeline\")\n",
    "\n",
    "print(f\"Random Forest pipeline saved to: {GOLD_DIR}/models/rf_pipeline\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "23fda988-620b-4c8c-b562-4bb965f7ef92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preds_lr ready with columns: ['itemid', 'visitorid', 'session_id', 'label', 'session_start', 'session_end', 'views', 'adds', 'unique_items', 'duration_min', 'u_events', 'u_views', 'u_adds', 'u_txn', 'i_views', 'i_adds', 'i_txn', 'categoryid', 'session_start_ts', 'categoryid_idx', 'categoryid_oh', 'features_raw', 'features', 'rawPrediction', 'probability', 'prediction', 'prob_arr', 'p1']\n"
     ]
    }
   ],
   "source": [
    "# Ensure preds_lr exists and contains class-1 probability as 'p1'\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.ml.functions import vector_to_array\n",
    "\n",
    "# If preds_lr doesn't exist yet, rebuild it from the model + test set\n",
    "if 'preds_lr' not in globals():\n",
    "    if 'model_lr' not in globals() or 'test' not in globals():\n",
    "        raise RuntimeError(\"Need 'model_lr' and 'test' in memory. Re-run training/split cells first.\")\n",
    "    preds_lr = model_lr.transform(test)\n",
    "\n",
    "# Add p1 from 'probability' vector if not already present\n",
    "cols = set(preds_lr.columns)\n",
    "if 'p1' not in cols:\n",
    "    if 'probability' in cols:\n",
    "        preds_lr = preds_lr.withColumn('prob_arr', vector_to_array('probability')) \\\n",
    "                           .withColumn('p1', F.col('prob_arr')[1]) \\\n",
    "                           .drop('prob_arr')\n",
    "    elif 'p_buy' in cols:\n",
    "        preds_lr = preds_lr.withColumn('p1', F.col('p_buy'))\n",
    "    else:\n",
    "        raise RuntimeError(\"preds_lr has neither 'probability' nor 'p_buy'. Re-run prediction to get probability scores.\")\n",
    "\n",
    "print(\"preds_lr ready with columns:\", preds_lr.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "49113c7e-be5d-429f-9f41-2cf29dc38a32",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/08/12 21:56:39 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/08/12 21:56:39 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/08/12 21:56:39 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/08/12 21:56:39 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/08/12 21:56:46 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/08/12 21:56:46 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/08/12 21:56:46 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/08/12 21:56:46 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/08/12 21:56:51 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote: gold/purchase_intent_top5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "GOLD_DIR = \"gold\"\n",
    "\n",
    "w = Window.partitionBy(\"session_id\").orderBy(F.col(\"p1\").desc())\n",
    "top5_lr = (preds_lr\n",
    "           .select(\"session_id\",\"itemid\",\"p1\")     # keep p1 for the window\n",
    "           .withColumn(\"rk\", F.row_number().over(w))\n",
    "           .filter(\"rk <= 5\")\n",
    "           .drop(\"rk\")\n",
    "           .withColumnRenamed(\"p1\", \"p_buy\"))      # rename after ranking\n",
    "\n",
    "top5_lr.write.mode(\"overwrite\").parquet(f\"{GOLD_DIR}/purchase_intent_top5\")\n",
    "print(f\"Wrote: {GOLD_DIR}/purchase_intent_top5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ddfbbcb3-974b-410b-b48f-5454ed436a05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preds_rf ready with columns: ['itemid', 'visitorid', 'session_id', 'label', 'session_start', 'session_end', 'views', 'adds', 'unique_items', 'duration_min', 'u_events', 'u_views', 'u_adds', 'u_txn', 'i_views', 'i_adds', 'i_txn', 'categoryid', 'session_start_ts', 'categoryid_idx', 'categoryid_oh', 'features_raw', 'features', 'rawPrediction', 'probability', 'prediction', 'prob_arr', 'p1']\n"
     ]
    }
   ],
   "source": [
    "# Ensure preds_rf exists and contains class-1 probability as 'p1'\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.ml.functions import vector_to_array\n",
    "\n",
    "# If preds_rf doesn't exist yet, rebuild it (from in-memory model) or load the saved pipeline\n",
    "if 'preds_rf' not in globals():\n",
    "    if 'model_rf' in globals() and 'test' in globals():\n",
    "        preds_rf = model_rf.transform(test)\n",
    "    else:\n",
    "        # Try loading from the saved pipeline\n",
    "        from pyspark.ml import PipelineModel\n",
    "        MODEL_DIR = \"gold/models/rf_pipeline\"\n",
    "        model_rf_loaded = PipelineModel.load(MODEL_DIR)\n",
    "        if 'test' not in globals():\n",
    "            raise RuntimeError(\"Need 'test' DataFrame to score. Re-run the split/feature cells to rebuild 'test'.\")\n",
    "        preds_rf = model_rf_loaded.transform(test)\n",
    "\n",
    "# Add p1 from 'probability' vector if not already present\n",
    "cols = set(preds_rf.columns)\n",
    "if 'p1' not in cols:\n",
    "    if 'probability' in cols:\n",
    "        preds_rf = preds_rf.withColumn('prob_arr', vector_to_array('probability')) \\\n",
    "                           .withColumn('p1', F.col('prob_arr')[1]) \\\n",
    "                           .drop('prob_arr')\n",
    "    elif 'p_buy' in cols:\n",
    "        preds_rf = preds_rf.withColumn('p1', F.col('p_buy'))\n",
    "    else:\n",
    "        raise RuntimeError(\"preds_rf has neither 'probability' nor 'p_buy'. Re-run prediction to get probability scores.\")\n",
    "\n",
    "print(\"preds_rf ready with columns:\", preds_rf.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8ef9b0e7-ea6e-47f1-9b93-6a8feed5b7e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/08/12 21:58:39 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/08/12 21:58:39 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/08/12 21:58:39 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/08/12 21:58:39 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/08/12 21:58:40 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/08/12 21:58:40 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/08/12 21:58:40 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/08/12 21:58:40 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/08/12 21:58:46 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/08/12 21:58:46 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/08/12 21:58:46 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/08/12 21:58:46 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/08/12 21:58:49 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "25/08/12 21:58:58 WARN DAGScheduler: Broadcasting large task binary with size 4.3 MiB\n",
      "25/08/12 21:58:59 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote: gold/purchase_intent_top5_rf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "GOLD_DIR = \"gold\"\n",
    "\n",
    "# Option B style: alias first, then order by p_buy\n",
    "df_rf = preds_rf.select(\"session_id\",\"itemid\", F.col(\"p1\").alias(\"p_buy\"))\n",
    "w = Window.partitionBy(\"session_id\").orderBy(F.col(\"p_buy\").desc())\n",
    "\n",
    "top5_rf = (df_rf\n",
    "           .withColumn(\"rk\", F.row_number().over(w))\n",
    "           .filter(\"rk <= 5\")\n",
    "           .drop(\"rk\"))\n",
    "\n",
    "top5_rf.write.mode(\"overwrite\").parquet(f\"{GOLD_DIR}/purchase_intent_top5_rf\")\n",
    "print(f\"Wrote: {GOLD_DIR}/purchase_intent_top5_rf\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78061d48-e191-402a-9aa3-93e0f45c788d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
